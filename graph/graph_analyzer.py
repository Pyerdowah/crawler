from itertools import combinations

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from scipy.stats import linregress
import random

from graph.custom_pagerank import custom_pagerank
from graph.visualisation import visualize_scc_graph


def analyze_connectivity_components(G):
    print("\nğŸ§© Analiza skÅ‚adowych spÃ³jnoÅ›ci")

    # WCC
    wcc = list(nx.weakly_connected_components(G))
    print(f" - Liczba sÅ‚abych skÅ‚adowych spÃ³jnoÅ›ci (WCC): {len(wcc)}")
    print(f" - Rozmiar najwiÄ™kszej WCC: {len(max(wcc, key=len))}")

    # SCC
    scc = list(nx.strongly_connected_components(G))
    print(f" - Liczba silnych skÅ‚adowych spÃ³jnoÅ›ci (SCC): {len(scc)}")
    largest_scc = max(scc, key=len)
    print(f" - Rozmiar najwiÄ™kszej SCC: {len(largest_scc)}")

    # IN/OUT/SCC/TENDRILS
    G_rev = G.reverse(copy=True)
    scc_set = set(largest_scc)

    reachable_from_scc = set()
    for node in largest_scc:
        reachable_from_scc.update(nx.descendants(G, node))
    reachable_from_scc.update(scc_set)

    reaching_scc = set()
    for node in largest_scc:
        reaching_scc.update(nx.descendants(G_rev, node))
    reaching_scc.update(scc_set)

    IN = reaching_scc - scc_set
    OUT = reachable_from_scc - scc_set
    TENDRILS = set(G.nodes()) - (scc_set | IN | OUT)

    print(f" - IN (prowadzÄ… do SCC): {len(IN)}")
    print(f" - OUT (osiÄ…galne z SCC): {len(OUT)}")
    print(f" - TENDRILS / ISLANDS (poza gÅ‚Ã³wnym korpusem): {len(TENDRILS)}")

    # Metagraf (graf SCC)
    G_scc = nx.condensation(G, scc=list(nx.strongly_connected_components(G)))
    print(f" - WierzchoÅ‚ki w G_SCC (grafie kondensacji): {G_scc.number_of_nodes()}")
    print(f" - KrawÄ™dzie w G_SCC: {G_scc.number_of_edges()}")
    visualize_scc_graph(G_scc)

    return {
        'wcc_count': len(wcc),
        'scc_count': len(scc),
        'largest_scc_size': len(largest_scc),
        'IN_size': len(IN),
        'OUT_size': len(OUT),
        'TENDRILS_size': len(TENDRILS),
        'G_SCC': G_scc
    }

def analyze_degree_distribution(G):
    print("\nğŸ“Š RozkÅ‚ady stopni (log-log + regresja):")

    for kind, deg_func in [("In-degree", G.in_degree), ("Out-degree", G.out_degree)]:
        degrees = [d for _, d in deg_func()]
        counts = Counter(degrees)

        # UsuÅ„ zera (log(0) = -inf)
        x_vals, y_vals = zip(*[(k, v) for k, v in counts.items() if k > 0 and v > 0])
        log_x = np.log10(x_vals)
        log_y = np.log10(y_vals)

        # Regresja log-log
        slope, intercept, r_value, _, _ = linregress(log_x, log_y)
        print(f" - {kind}: y = x^{slope:.2f}, RÂ² = {r_value**2:.3f}")

        # Wykres log-log
        plt.figure(figsize=(6, 4))
        plt.loglog(x_vals, y_vals, marker='o', linestyle='None', label='Dane')
        plt.plot(x_vals, 10**(intercept + slope * log_x), label=f'Regresja: x^{slope:.2f}')
        plt.title(f"RozkÅ‚ad {kind}")
        plt.xlabel("StopieÅ„")
        plt.ylabel("Liczba wierzchoÅ‚kÃ³w")
        plt.grid(True, which="both", ls="--")
        plt.legend()
        plt.tight_layout()
        plt.show()

def analyze_shortest_paths(G):
    print("\nğŸ§­ Analiza najkrÃ³tszych Å›cieÅ¼ek (dla najwiÄ™kszej SCC):")

    # ZnajdÅº najwiÄ™kszÄ… SCC
    scc = max(nx.strongly_connected_components(G), key=len)
    H = G.subgraph(scc).copy()

    # OdlegÅ‚oÅ›ci miÄ™dzy wszystkimi parami
    all_lengths = dict(nx.all_pairs_shortest_path_length(H))

    node_avg_dists = []
    node_radii = []

    all_distances = []

    for node, targets in all_lengths.items():
        dists = list(targets.values())
        all_distances.extend(dists)
        node_avg_dists.append(np.mean(dists))
        node_radii.append(max(dists))

    avg_path_len = np.mean(all_distances)
    diameter = max(all_distances)

    print(f" - Åšrednia odlegÅ‚oÅ›Ä‡ (dla SCC): {avg_path_len:.2f}")
    print(f" - Åšrednica (diameter): {diameter}")

    # Histogram Å›rednich odlegÅ‚oÅ›ci z kaÅ¼dego wierzchoÅ‚ka
    plt.figure()
    plt.hist(node_avg_dists, bins=30)
    plt.title("Histogram Å›rednich odlegÅ‚oÅ›ci z wierzchoÅ‚kÃ³w")
    plt.xlabel("Åšrednia odlegÅ‚oÅ›Ä‡")
    plt.ylabel("Liczba wierzchoÅ‚kÃ³w")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Histogram promieni
    plt.figure()
    plt.hist(node_radii, bins=30)
    plt.title("Histogram promieni (maksymalnych odlegÅ‚oÅ›ci)")
    plt.xlabel("PromieÅ„")
    plt.ylabel("Liczba wierzchoÅ‚kÃ³w")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Regresja log-log histogramu Å›rednich odlegÅ‚oÅ›ci
    counts = np.bincount(np.array(node_avg_dists, dtype=int))
    x_vals = np.arange(len(counts))
    y_vals = counts

    mask = (x_vals > 0) & (y_vals > 0)
    log_x = np.log10(x_vals[mask])
    log_y = np.log10(y_vals[mask])

    if len(log_x) > 1:
        slope, intercept, r_value, _, _ = linregress(log_x, log_y)
        print(f" - Regresja histogramu (Å›rednie odlegÅ‚oÅ›ci): x^{slope:.2f}, RÂ²={r_value ** 2:.2f}")
    else:
        print(" - Za maÅ‚o danych do regresji histogramu.")

    return {
        'avg_path_len': avg_path_len,
        'diameter': diameter,
        'avg_dists': node_avg_dists,
        'radii': node_radii
    }

def analyze_clustering(G):
    print("\nğŸ”— WspÃ³Å‚czynniki klasteryzacji:")

    UG = G.to_undirected()
    clustering = nx.clustering(UG)
    values = list(clustering.values())

    global_clustering = nx.average_clustering(UG)
    print(f" - Globalna klasteryzacja: {global_clustering:.4f}")
    print(f" - Liczba wierzchoÅ‚kÃ³w z C > 0: {sum(c > 0 for c in values)}")

    # Histogram lokalnych wartoÅ›ci
    plt.figure()
    plt.hist(values, bins=30)
    plt.title("Histogram lokalnych wspÃ³Å‚czynnikÃ³w klasteryzacji")
    plt.xlabel("WspÃ³Å‚czynnik klasteryzacji")
    plt.ylabel("Liczba wierzchoÅ‚kÃ³w")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Przygotowanie do regresji â€” histogram (binning) wartoÅ›ci > 0
    nonzero = [v for v in values if v > 0]
    bins = np.linspace(0.01, 1.0, 30)
    hist, edges = np.histogram(nonzero, bins=bins)

    x_vals = 0.5 * (edges[:-1] + edges[1:])
    y_vals = hist

    mask = (y_vals > 0)
    log_x = np.log10(x_vals[mask])
    log_y = np.log10(y_vals[mask])

    if len(log_x) > 1:
        slope, intercept, r_value, _, _ = linregress(log_x, log_y)
        print(f" - Regresja histogramu klasteryzacji: x^{slope:.2f}, RÂ²={r_value**2:.2f}")

        # Wykres log-log histogramu z regresjÄ…
        plt.figure()
        plt.loglog(x_vals[mask], y_vals[mask], 'o', label='Dane')
        plt.loglog(x_vals[mask], 10**(intercept + slope * log_x), label=f'Regresja: x^{slope:.2f}')
        plt.title("RozkÅ‚ad lokalnej klasteryzacji (log-log)")
        plt.xlabel("WspÃ³Å‚czynnik klasteryzacji")
        plt.ylabel("Liczba wierzchoÅ‚kÃ³w")
        plt.legend()
        plt.grid(True, which="both", ls="--")
        plt.tight_layout()
        plt.show()
    else:
        print(" - Za maÅ‚o danych do regresji klasteryzacji.")

def analyze_vertex_connectivity(G):
    print("\nğŸ§© SpÃ³jnoÅ›Ä‡ wierzchoÅ‚kowa:")

    UG = G.to_undirected()
    if not nx.is_connected(UG):
        print(" - Graf niespÃ³jny â†’ brak sensu szukaÄ‡ rozspajajÄ…cych wierzchoÅ‚kÃ³w.")
        return

    connectivity = nx.node_connectivity(UG)
    print(f" - SpÃ³jnoÅ›Ä‡ wierzchoÅ‚kowa: {connectivity}")

    if connectivity == 1:
        print(" - Szukanie wierzchoÅ‚kÃ³w rozspajajÄ…cych (punkty artykulacji)...")
        articulation = list(nx.articulation_points(UG))
        print(f"   Znaleziono {len(articulation)} punktÃ³w artykulacji.")
        print("   PrzykÅ‚ad:", articulation[:5])
    elif connectivity == 2:
        print(" - Szukanie par wierzchoÅ‚kÃ³w rozspajajÄ…cych (moÅ¼e byÄ‡ kosztowne)...")
        cut_pairs = []
        nodes = list(UG.nodes())
        for u, v in combinations(nodes, 2):
            UG_copy = UG.copy()
            UG_copy.remove_nodes_from([u, v])
            if not nx.is_connected(UG_copy):
                cut_pairs.append((u, v))
                if len(cut_pairs) >= 5:
                    break
        print(f"   PrzykÅ‚adowe pary rozspajajÄ…ce: {cut_pairs}")
    else:
        print(" - Graf ma spÃ³jnoÅ›Ä‡ â‰¥ 3 â†’ brak rozspajajÄ…cych pojedynczych/podwÃ³jnych.")

def analyze_pagerank_distribution(pr_dict, title="RozkÅ‚ad PageRank"):
    import matplotlib.pyplot as plt
    from collections import Counter
    from scipy.stats import linregress
    import numpy as np

    values = sorted(pr_dict.values(), reverse=True)
    ranks = list(range(1, len(values) + 1))

    plt.figure()
    plt.loglog(ranks, values, marker='o', linestyle='None')
    plt.title(title)
    plt.xlabel("Ranga")
    plt.ylabel("PageRank")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Regresja log-log
    log_x = np.log10(ranks)
    log_y = np.log10(values)
    slope, intercept, r_value, _, _ = linregress(log_x, log_y)
    print(f" - Regresja log-log: y ~ x^{slope:.2f}, RÂ² = {r_value**2:.3f}")

def pagerank_convergence_study(G, d_values=[0.6, 0.75, 0.85, 0.95, 1.0]):
    print("\nğŸ” ZbieÅ¼noÅ›Ä‡ PageRank dla rÃ³Å¼nych wspÃ³Å‚czynnikÃ³w tÅ‚umienia:")
    for d in d_values:
        print(f"\nDamping = {d}")
        pr = custom_pagerank(G, d=d, verbose=True)
        top = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:5]
        print("Top 5 stron:")
        for node, val in top:
            print(f"  {node[:60]}... : {val:.4e}")
        analyze_pagerank_distribution(pr, title=f"PageRank (d={d})")




